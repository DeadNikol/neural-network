{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792817a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NN:\n",
    "    def __init__(self):\n",
    "        self.layersOfWeights = [] #Слои нейронов\n",
    "        self.layersOfBiases = [] #Слои смещений\n",
    "        self.activFunc = [] #Слои функций активации (Мало ли они будут разные)\n",
    "        self.gradActivFunc = [] #Производная от слоя активации. Нужна на случай, если таковой нет(Тут будет просто функция, которая возвращает вход)\n",
    "        self.countOfLayers = 0 #Просто число слоёв\n",
    "\n",
    "        self.calculatedBeforeActivFunc = [] #Промежуточные значения до функции активации на каждом слое\n",
    "        self.calculatedAfterActivFunc = [] #Промежуточные знчаения после вычисления фунцкции активации на каждом слое\n",
    "\n",
    "        self.errorsDuringTraining = [] #Список ошибок на каждом шагу обучения\n",
    "\n",
    "    def addLayer(self, n, m):\n",
    "        \"\"\"\n",
    "        n - входное число нейронов<br>\n",
    "        m - выходное число нейронов\n",
    "        \"\"\"\n",
    "        q = 1\n",
    "        self.layersOfWeights.append(np.random.uniform(-q, q, (n,m)))\n",
    "        self.layersOfBiases.append(np.random.uniform(-q, q, (1,m)))\n",
    "        self.countOfLayers += 1\n",
    "\n",
    "    def addActivFunc(self, n:int=0):\n",
    "        \"\"\"\n",
    "        n - номер функции активации: <br>\n",
    "        <b>0</b> - никакой функции активации. Выбор по умолчанию<br>\n",
    "        <b>1</b> - сигмойда (1/1 + exp(-x))<br>\n",
    "        <b>2</b> - гиперболический тангенс (tanh(x))<br>\n",
    "        <b>3</b> - softmax (exp(xi)) / sum(exp(xi))\n",
    "        \"\"\"\n",
    "        none = lambda x: x\n",
    "        gradNone = lambda x: np.ones_like(x)\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        gradSigmoid = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        gradTanh = lambda x: 1 - (np.tanh(x))**2\n",
    "\n",
    "        def softmax(x:np.ndarray):\n",
    "            \"\"\"x - вектор предсказаний\"\"\"\n",
    "            x = np.exp(x)\n",
    "            denuminator = x.sum(axis=1).reshape(x.shape[0], 1)\n",
    "            return x / denuminator\n",
    "\n",
    "        def gradSoftmax(b:np.ndarray):\n",
    "            \"\"\"Градиент из softmax'a. <br>\n",
    "            На вход принимает матрицу значений softmax, где строки - это один набор данных, а столбцы - значения выходных нейронов\"\"\"\n",
    "            z = -b[:, :, None] * b[:, None]\n",
    "            diag = np.diag_indices_from(z[0])\n",
    "            z[:, diag[0], diag[1]] = b[:] * (1 - b[:])\n",
    "            return z.sum(axis=1)\n",
    "\n",
    "        if n ==0:\n",
    "            func = none\n",
    "            gradFunc = gradNone\n",
    "        elif n ==1:\n",
    "            func = sigmoid\n",
    "            gradFunc = gradSigmoid\n",
    "        elif n == 2: \n",
    "            func = tanh\n",
    "            gradFunc = gradTanh\n",
    "        elif n == 3:\n",
    "            func = softmax\n",
    "            gradFunc = gradSoftmax\n",
    "        else:\n",
    "            raise Exception(\"Выбрана неверная функция активации\")\n",
    "        \n",
    "        self.activFunc.append(func)\n",
    "        self.gradActivFunc.append(gradFunc)\n",
    "\n",
    "    def predict(self, data:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        data - данные, на основе которых мы хотим получить предсказание\n",
    "        \"\"\"\n",
    "        self.calculatedAfterActivFunc.append(data)\n",
    "        a = data @ self.layersOfWeights[0] + self.layersOfBiases[0] #Первый слой\n",
    "        self.calculatedBeforeActivFunc.append(a)\n",
    "        a = self.activFunc[0](a)\n",
    "        self.calculatedAfterActivFunc.append(a)\n",
    "        \n",
    "        for i in range(1, self.countOfLayers): #Все последующие слои, начиная со второго\n",
    "            a = a @ self.layersOfWeights[i] + self.layersOfBiases[i]\n",
    "            self.calculatedBeforeActivFunc.append(a)\n",
    "            a = self.activFunc[i](a)\n",
    "            self.calculatedAfterActivFunc.append(a)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def _predict(self, data:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        <h3>Этот метод нужен для подсчёта внутри процесса обучения, что бы не влиять на него</h3>\n",
    "        data - данные, на основе которых мы хотим получить предсказание\n",
    "        \"\"\"\n",
    "        a = data @ self.layersOfWeights[0] + self.layersOfBiases[0] #Первый слой\n",
    "        a = self.activFunc[0](a)\n",
    "        if self.countOfLayers != 1:\n",
    "            for i in range(1, self.countOfLayers): #Все последующие слои, начиная со второго\n",
    "                a = a @ self.layersOfWeights[i] + self.layersOfBiases[i]\n",
    "                a = self.activFunc[i](a)\n",
    "        return a\n",
    "    \n",
    "    def _backProp(self, data:np.ndarray, true:np.ndarray, lr):\n",
    "        \"\"\"\n",
    "        Метод обучения. Принимает в себя <b>(потенциально один)</b> экземпляр, на котором и учится \n",
    "        \"\"\"\n",
    "        self.calculatedBeforeActivFunc.clear()\n",
    "        self.calculatedAfterActivFunc.clear()\n",
    "        pred = self._predict(data)\n",
    "\n",
    "        # e = ((pred-true)**2).mean()\n",
    "\n",
    "        dedy = (2 * (pred - true).mean()).reshape(1,1) #Это теперь одно число\n",
    "        self.dedy.append(dedy)\n",
    "\n",
    "        #Первый слой\n",
    "        dydz = dedy * self.gradActivFunc[-1](self.calculatedBeforeActivFunc[-1])\n",
    "        self.dydz.append(dydz)\n",
    "\n",
    "        dzdw = self.calculatedAfterActivFunc[-2].T @ dydz \n",
    "        self.dzdw.append(dzdw)\n",
    "\n",
    "        dzdb = (dydz * 1).mean(axis=0)\n",
    "        self.dzdb.append(dzdb)\n",
    "\n",
    "        dzdy = dydz @ self.layersOfWeights[-1].T #updated\n",
    "        self.dzdy.append(dzdy)\n",
    "\n",
    "        self.layersOfWeights[-1] = self.layersOfWeights[-1] - dzdw * lr\n",
    "        self.layersOfBiases[-1] = self.layersOfBiases[-1] - dzdb * lr\n",
    "\n",
    "        #Все последующие слои, начиная со второго\n",
    "        for i in range(1, self.countOfLayers):\n",
    "            dydz = dzdy * self.gradActivFunc[-i-1](self.calculatedBeforeActivFunc[-i-1])\n",
    "            dzdw = self.calculatedAfterActivFunc[-i-2].T @ dydz\n",
    "            dzdb = (dydz * 1).mean(axis=0)\n",
    "            self.layersOfBiases[-i-1] = self.layersOfBiases[-i-1] - dzdb * lr\n",
    "            dzdy = dydz @ self.layersOfWeights[-i-1].T\n",
    "            self.layersOfWeights[-i-1] = self.layersOfWeights[-i-1] - dzdw * lr #updated\n",
    "\n",
    "            # pred = self._predict(data)\n",
    "            # e = ((pred-true)**2).mean()\n",
    "            # self.errorsDuringTraining.append(e)\n",
    "        \n",
    "        pred = self._predict(data)\n",
    "        e = ((pred-true)**2).mean()\n",
    "        self.errorsDuringTraining.append(e)\n",
    "\n",
    "    def training(self, data:np.ndarray, true:np.ndarray, steps, lr = 0.001, stopCreteria = 0.001, chart = False):\n",
    "        self.errorsDuringTraining.clear()\n",
    "\n",
    "        self.dedy = []\n",
    "        self.dydz = []\n",
    "        self.dzdw = []\n",
    "        self.dzdb = []\n",
    "        self.dzdy = []\n",
    "\n",
    "        for i in range(steps):\n",
    "            self._backProp(data, true, lr)\n",
    "            if len(self.errorsDuringTraining) >= 2:\n",
    "                if (self.errorsDuringTraining[-2] - self.errorsDuringTraining[-1]) < stopCreteria:\n",
    "                    print(f'Всего за {i} шагов модель обучилась достаточно')\n",
    "                    break\n",
    "        if chart:\n",
    "            plt.figure()\n",
    "            plt.title(\"График обучения\")\n",
    "            plt.plot(range(self.errorsDuringTraining.__len__()), self.errorsDuringTraining)\n",
    "            plt.xlabel(\"Номер итерации\")\n",
    "            plt.ylabel(\"Ошибка\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
