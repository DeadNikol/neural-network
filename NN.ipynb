{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792817a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NN:\n",
    "    def __init__(self):\n",
    "        self.layersOfWeights = [] #Слои нейронов\n",
    "        self.layersOfBiases = [] #Слои смещений\n",
    "        self.activFunc = [] #Слои функций активации (Мало ли они будут разные)\n",
    "        self.gradActivFunc = [] #Производная от слоя активации. Нужна на случай, если таковой нет(Тут будет просто функция, которая возвращает вход)\n",
    "        self.countOfLayers = 0 #Просто число слоёв\n",
    "\n",
    "        self.calculatedBeforeActivFunc = [] #Промежуточные значения до функции активации на каждом слое\n",
    "        self.calculatedAfterActivFunc = [] #Промежуточные знчаения после вычисления фунцкции активации на каждом слое\n",
    "\n",
    "        self.errorsDuringTraining = [] #Список ошибок на каждом шагу обучения\n",
    "\n",
    "    def addLayer(self, n, m):\n",
    "        \"\"\"\n",
    "        n - входное число нейронов<br>\n",
    "        m - выходное число нейронов\n",
    "        \"\"\"\n",
    "        q = 0.1\n",
    "        self.layersOfWeights.append(np.random.uniform(-q, q, (n,m)))\n",
    "        self.layersOfBiases.append(np.random.uniform(-q, q, (1,m)))\n",
    "        self.countOfLayers += 1\n",
    "\n",
    "    def addActivFunc(self, n:int=0):\n",
    "        \"\"\"\n",
    "        n - номер функции активации: <br>\n",
    "        <b>0</b> - никакой функции активации. Выбор по умолчанию<br>\n",
    "        <b>1</b> - сигмойда (1/1 + exp(-x))<br>\n",
    "        <b>2</b> - гиперболический тангенс (tanh(x))<br>\n",
    "        <b>3</b> - softmax (exp(xi)) / sum(exp(xi))\n",
    "        \"\"\"\n",
    "        none = lambda x: x\n",
    "        gradNone = lambda x: np.ones_like(x)\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        gradSigmoid = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        gradTanh = lambda x: 1 - (np.tanh(x))**2\n",
    "\n",
    "        def softmax(x):\n",
    "            # shift = x - np.max(x, axis=1, keepdims=True)\n",
    "            # ex = np.exp(shift)\n",
    "            # return ex / np.sum(ex, axis=1, keepdims=True)\n",
    "        \n",
    "            shift = x - np.max(x, axis=1, keepdims=True)\n",
    "            shift = np.clip(shift, -20, +20)\n",
    "            ex = np.exp(shift)\n",
    "            return ex / np.sum(ex, axis=1, keepdims=True)\n",
    "\n",
    "        def gradSoftmax(b:np.ndarray):\n",
    "            \"\"\"Градиент из softmax'a. <br>\n",
    "            На вход принимает матрицу значений softmax, где строки - это один набор данных, а столбцы - значения выходных нейронов\"\"\"\n",
    "            z = -b[:, :, None] * b[:, None]\n",
    "            diag = np.diag_indices_from(z[0])\n",
    "            z[:, diag[0], diag[1]] = b[:] * (1 - b[:])\n",
    "            return z.sum(axis=1)\n",
    "\n",
    "        if n ==0:\n",
    "            func = none\n",
    "            gradFunc = gradNone\n",
    "        elif n ==1:\n",
    "            func = sigmoid\n",
    "            gradFunc = gradSigmoid\n",
    "        elif n == 2: \n",
    "            func = tanh\n",
    "            gradFunc = gradTanh\n",
    "        elif n == 3:\n",
    "            func = softmax\n",
    "            gradFunc = gradSoftmax\n",
    "        else:\n",
    "            raise Exception(\"Выбрана неверная функция активации\")\n",
    "        \n",
    "        self.activFunc.append(func)\n",
    "        self.gradActivFunc.append(gradFunc)\n",
    "\n",
    "    def predict(self, data:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        data - данные, на основе которых мы хотим получить предсказание\n",
    "        \"\"\"\n",
    "        self.calculatedAfterActivFunc.append(data)\n",
    "        a = data @ self.layersOfWeights[0] + self.layersOfBiases[0] #Первый слой\n",
    "        self.calculatedBeforeActivFunc.append(a)\n",
    "        a = self.activFunc[0](a)\n",
    "        self.calculatedAfterActivFunc.append(a)\n",
    "        \n",
    "        for i in range(1, self.countOfLayers): #Все последующие слои, начиная со второго\n",
    "            a = a @ self.layersOfWeights[i] + self.layersOfBiases[i]\n",
    "            self.calculatedBeforeActivFunc.append(a)\n",
    "            a = self.activFunc[i](a)\n",
    "            self.calculatedAfterActivFunc.append(a)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def _predict(self, data:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        <h3>Этот метод нужен для подсчёта внутри процесса обучения, что бы не влиять на него</h3>\n",
    "        data - данные, на основе которых мы хотим получить предсказание\n",
    "        \"\"\"\n",
    "        a = data @ self.layersOfWeights[0] + self.layersOfBiases[0] #Первый слой\n",
    "        a = self.activFunc[0](a)\n",
    "        if self.countOfLayers != 1:\n",
    "            for i in range(1, self.countOfLayers): #Все последующие слои, начиная со второго\n",
    "                a = a @ self.layersOfWeights[i] + self.layersOfBiases[i]\n",
    "                a = self.activFunc[i](a)\n",
    "        return a\n",
    "    \n",
    "    def _backProp(self, data:np.ndarray, true:np.ndarray, lr):\n",
    "        \"\"\"\n",
    "        Метод обучения. Принимает в себя <b>(потенциально один)</b> экземпляр, на котором и учится \n",
    "        \"\"\"\n",
    "        self.calculatedBeforeActivFunc.clear()\n",
    "        self.calculatedAfterActivFunc.clear()\n",
    "        pred = self.predict(data) # Модель не обучалась потому что у тебя в предикте не заполнялись данные с прямого прохода\n",
    "        \n",
    "        e = ((pred-true)**2).mean()\n",
    "        self.errorsDuringTraining.append(e)\n",
    "        \n",
    "        dedy = 2*(pred - true) / (pred.shape[0] * pred.shape[1]) # Тут ты видимо просто закомментил вычисление градиента\n",
    "        self.dedy.append(dedy)\n",
    "\n",
    "        #Первый слой\n",
    "        dydz = dedy * self.gradActivFunc[-1](self.calculatedBeforeActivFunc[-1])\n",
    "        self.dydz.append(dydz)\n",
    "\n",
    "        dzdw = self.calculatedAfterActivFunc[-2].T @ dydz \n",
    "        self.dzdw.append(dzdw)\n",
    "\n",
    "        dzdb = (dydz * 1).mean(axis=0)\n",
    "        self.dzdb.append(dzdb)\n",
    "\n",
    "        dzdy = dydz @ self.layersOfWeights[-1].T #updated\n",
    "        self.dzdy.append(dzdy)\n",
    "\n",
    "        self.layersOfWeights[-1] = self.layersOfWeights[-1] - dzdw * lr\n",
    "        self.layersOfBiases[-1] = self.layersOfBiases[-1] - dzdb * lr\n",
    "\n",
    "        #Все последующие слои, начиная со второго\n",
    "        for i in range(1, self.countOfLayers):\n",
    "            dydz = dzdy * self.gradActivFunc[-i-1](self.calculatedBeforeActivFunc[-i-1])\n",
    "            dzdw = self.calculatedAfterActivFunc[-i-2].T @ dydz\n",
    "            dzdb = (dydz * 1).mean(axis=0)\n",
    "            self.layersOfBiases[-i-1] = self.layersOfBiases[-i-1] - dzdb * lr\n",
    "            dzdy = dydz @ self.layersOfWeights[-i-1].T\n",
    "            self.layersOfWeights[-i-1] = self.layersOfWeights[-i-1] - dzdw * lr #updated\n",
    "\n",
    "\n",
    "    def training(self, data:np.ndarray, true:np.ndarray, steps, lr = 0.001, stopCreteria = 0.001, chart = False):\n",
    "        self.errorsDuringTraining.clear()\n",
    "\n",
    "        self.dedy = []\n",
    "        self.dydz = []\n",
    "        self.dzdw = []\n",
    "        self.dzdb = []\n",
    "        self.dzdy = []\n",
    "\n",
    "        for i in range(steps):\n",
    "            self._backProp(data, true, lr)\n",
    "            # # Тут я даже не разбирался, ошибка в логике постоянно прерывала обучение.\n",
    "            # if len(self.errorsDuringTraining) >= 2:\n",
    "            #     if (self.errorsDuringTraining[-2] - self.errorsDuringTraining[-1]) < stopCreteria:\n",
    "            #         print(f'Всего за {i} шагов модель обучилась достаточно')\n",
    "            #         break\n",
    "        if chart:\n",
    "            plt.figure()\n",
    "            plt.title(\"График обучения\")\n",
    "            plt.plot(range(self.errorsDuringTraining.__len__()), self.errorsDuringTraining)\n",
    "            plt.xlabel(\"Номер итерации\")\n",
    "            plt.ylabel(\"Ошибка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Определение модели\"\"\"\n",
    "\n",
    "net = NN()\n",
    "# Входной слой: 4 признака → 10 нейронов, скрытый слой → 3 нейрона на выходе\n",
    "net.addLayer(4, 10)\n",
    "net.addActivFunc(2)   # tanh\n",
    "net.addLayer(10, 3)\n",
    "net.addActivFunc(3)   # softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feebeecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Загружаем датасет\"\"\"\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 1. Загружаем датасет Iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data               # shape (150, 4)\n",
    "y = iris.target.reshape(-1, 1)  # shape (150, 1)\n",
    "\n",
    "# 2. One‑hot кодирование меток (3 класса)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "Y = encoder.fit_transform(y)  # shape (150, 3)\n",
    "\n",
    "# 3. Разбиваем на тренировочную и тестовую выборки\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "# 4. Нормализуем вход\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform (X_test)\n",
    "\n",
    "# 6. Оцениваем точность до обучения\n",
    "pred_probs = net.predict(X_test)\n",
    "pred_labels_nt = np.argmax(pred_probs, axis=1)\n",
    "true_labels = np.argmax(Y_test, axis=1)\n",
    "accuracy = (pred_labels_nt == true_labels).mean()\n",
    "print(f\"Start accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Обучаем\n",
    "net.training(X_train, Y_train,\n",
    "             steps=20000,\n",
    "             lr=0.001,\n",
    "             stopCreteria=1e-5,\n",
    "             chart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce00310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Оцениваем точность на тесте\n",
    "pred_probs = net.predict(X_test)\n",
    "pred_labels = np.argmax(pred_probs, axis=1)\n",
    "true_labels = np.argmax(Y_test, axis=1)\n",
    "accuracy = (pred_labels == true_labels).mean()\n",
    "print(f\"Test accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим как изменились метки после обучения\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 5) Собираем DataFrame\n",
    "df_results = pd.DataFrame({\n",
    "    'TrueLabel':   true_labels,\n",
    "    'PredBefore':  pred_labels_nt,\n",
    "    'PredAfter':   pred_labels\n",
    "})\n",
    "\n",
    "df_results['Changed?'] = df_results['PredBefore'] != df_results['PredAfter']\n",
    "\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d27519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# число прогонов для каждой комбинации (чем больше, тем точнее, но дольше)\n",
    "n_repeats = 5\n",
    "\n",
    "# 2) Сетка гиперпараметров\n",
    "layer_configs = [\n",
    "    [10],        # одна скрытая из 10 нейронов\n",
    "    [20],        # одна скрытая из 20\n",
    "    [10, 10],    # две по 10\n",
    "    [3, 5, 3],\n",
    "]\n",
    "activ_choices = {\n",
    "    1: \"sigmoid\",\n",
    "    2: \"tanh\",\n",
    "    3: \"softmax\"\n",
    "}\n",
    "weight_scales = [0.1, 1.0, 0.01]\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# 3) Grid search с повторениями\n",
    "for hidden, act_code, scale, lr in itertools.product(\n",
    "        layer_configs, activ_choices, weight_scales, lrs):\n",
    "\n",
    "    accs = []\n",
    "    for run in range(n_repeats):\n",
    "        # фиксируем семя, чтобы разные запуски отличались, но было воспроизводимо\n",
    "        seed = 42 + run\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        net = NN()\n",
    "        prev_dim = X_train.shape[1]\n",
    "        # создаём скрытые слои\n",
    "        for size in hidden:\n",
    "            net.addLayer(prev_dim, size)\n",
    "            net.addActivFunc(act_code)\n",
    "            prev_dim = size\n",
    "        # выходной softmax-слой\n",
    "        net.addLayer(prev_dim, Y_train.shape[1])\n",
    "        net.addActivFunc(3)\n",
    "        # масштабируем стартовые веса\n",
    "        for i in range(len(net.layersOfWeights)):\n",
    "            net.layersOfWeights[i] *= scale\n",
    "\n",
    "        # обучаем\n",
    "        net.training(\n",
    "            X_train, Y_train,\n",
    "            steps=200,\n",
    "            lr=lr,\n",
    "            chart=False\n",
    "        )\n",
    "\n",
    "        # валидация\n",
    "        pred = net.predict(X_test)\n",
    "        pred_labels = np.argmax(pred, axis=1)\n",
    "        true_labels = np.argmax(Y_test, axis=1)\n",
    "        accs.append((pred_labels == true_labels).mean())\n",
    "\n",
    "    # после n_repeats считаем метрики стабильности\n",
    "    mean_acc = np.mean(accs)\n",
    "    std_acc  = np.std(accs)\n",
    "\n",
    "    results.append({\n",
    "        \"hidden_layers\": hidden,\n",
    "        \"activation\":   activ_choices[act_code],\n",
    "        \"weight_scale\": scale,\n",
    "        \"learning_rate\": lr,\n",
    "        \"mean_acc\":     mean_acc,\n",
    "        \"std_acc\":      std_acc,\n",
    "        \"all_runs\":     accs\n",
    "    })\n",
    "\n",
    "# 4) Сводная таблица: сортируем по убыванию mean_acc, затем по возрастанию std_acc\n",
    "df = pd.DataFrame(results)\n",
    "df = df.sort_values([\"mean_acc\", \"std_acc\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "# посмотрим топ-10 самых стабильных и высоко среднеработающих конфигураций\n",
    "df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
